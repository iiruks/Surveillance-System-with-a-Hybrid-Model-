{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eef1269a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SwinForImageClassification were not initialized from the model checkpoint at microsoft/swin-tiny-patch4-window7-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([101, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([101]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [10/395], Loss: 2.0471, Accuracy: 59.38%\n",
      "Epoch [1/3], Step [20/395], Loss: 1.5452, Accuracy: 68.75%\n",
      "Epoch [1/3], Step [30/395], Loss: 0.9077, Accuracy: 69.79%\n",
      "Epoch [1/3], Step [40/395], Loss: 0.5226, Accuracy: 73.28%\n",
      "Epoch [1/3], Step [50/395], Loss: 1.2386, Accuracy: 74.12%\n",
      "Epoch [1/3], Step [60/395], Loss: 0.5527, Accuracy: 74.90%\n",
      "Epoch [1/3], Step [70/395], Loss: 1.3703, Accuracy: 74.55%\n",
      "Epoch [1/3], Step [80/395], Loss: 1.0352, Accuracy: 74.38%\n",
      "Epoch [1/3], Step [90/395], Loss: 1.2353, Accuracy: 74.51%\n",
      "Epoch [1/3], Step [100/395], Loss: 0.9099, Accuracy: 74.81%\n",
      "Epoch [1/3], Step [110/395], Loss: 1.4807, Accuracy: 75.45%\n",
      "Epoch [1/3], Step [120/395], Loss: 0.9040, Accuracy: 75.89%\n",
      "Epoch [1/3], Step [130/395], Loss: 0.8414, Accuracy: 75.77%\n",
      "Epoch [1/3], Step [140/395], Loss: 1.1673, Accuracy: 75.67%\n",
      "Epoch [1/3], Step [150/395], Loss: 1.6101, Accuracy: 75.29%\n",
      "Epoch [1/3], Step [160/395], Loss: 0.5200, Accuracy: 75.20%\n",
      "Epoch [1/3], Step [170/395], Loss: 1.4906, Accuracy: 75.18%\n",
      "Epoch [1/3], Step [180/395], Loss: 1.1298, Accuracy: 74.90%\n",
      "Epoch [1/3], Step [190/395], Loss: 1.2196, Accuracy: 74.87%\n",
      "Epoch [1/3], Step [200/395], Loss: 1.4895, Accuracy: 75.09%\n",
      "Epoch [1/3], Step [210/395], Loss: 1.2931, Accuracy: 75.36%\n",
      "Epoch [1/3], Step [220/395], Loss: 1.2247, Accuracy: 75.11%\n",
      "Epoch [1/3], Step [230/395], Loss: 0.9993, Accuracy: 74.95%\n",
      "Epoch [1/3], Step [240/395], Loss: 1.1164, Accuracy: 74.90%\n",
      "Epoch [1/3], Step [250/395], Loss: 1.3351, Accuracy: 74.80%\n",
      "Epoch [1/3], Step [260/395], Loss: 0.2590, Accuracy: 74.98%\n",
      "Epoch [1/3], Step [270/395], Loss: 0.7380, Accuracy: 74.88%\n",
      "Epoch [1/3], Step [280/395], Loss: 1.3827, Accuracy: 74.98%\n",
      "Epoch [1/3], Step [290/395], Loss: 1.1922, Accuracy: 75.06%\n",
      "Epoch [1/3], Step [300/395], Loss: 1.8419, Accuracy: 74.96%\n",
      "Epoch [1/3], Step [310/395], Loss: 1.9556, Accuracy: 74.82%\n",
      "Epoch [1/3], Step [320/395], Loss: 0.6841, Accuracy: 74.98%\n",
      "Epoch [1/3], Step [330/395], Loss: 1.0264, Accuracy: 75.13%\n",
      "Epoch [1/3], Step [340/395], Loss: 1.1550, Accuracy: 75.20%\n",
      "Epoch [1/3], Step [350/395], Loss: 1.8509, Accuracy: 75.02%\n",
      "Epoch [1/3], Step [360/395], Loss: 1.4174, Accuracy: 74.97%\n",
      "Epoch [1/3], Step [370/395], Loss: 0.6080, Accuracy: 74.98%\n",
      "Epoch [1/3], Step [380/395], Loss: 0.7549, Accuracy: 75.00%\n",
      "Epoch [1/3], Step [390/395], Loss: 1.1846, Accuracy: 74.86%\n",
      "Epoch [2/3], Step [10/395], Loss: 1.9128, Accuracy: 72.50%\n",
      "Epoch [2/3], Step [20/395], Loss: 1.2214, Accuracy: 73.12%\n",
      "Epoch [2/3], Step [30/395], Loss: 0.6800, Accuracy: 74.17%\n",
      "Epoch [2/3], Step [40/395], Loss: 0.7713, Accuracy: 74.38%\n",
      "Epoch [2/3], Step [50/395], Loss: 1.3168, Accuracy: 74.50%\n",
      "Epoch [2/3], Step [60/395], Loss: 1.2722, Accuracy: 73.96%\n",
      "Epoch [2/3], Step [70/395], Loss: 1.1788, Accuracy: 74.11%\n",
      "Epoch [2/3], Step [80/395], Loss: 1.1554, Accuracy: 73.83%\n",
      "Epoch [2/3], Step [90/395], Loss: 1.2007, Accuracy: 74.31%\n",
      "Epoch [2/3], Step [100/395], Loss: 1.4651, Accuracy: 74.44%\n",
      "Epoch [2/3], Step [110/395], Loss: 0.9201, Accuracy: 74.89%\n",
      "Epoch [2/3], Step [120/395], Loss: 2.0562, Accuracy: 75.26%\n",
      "Epoch [2/3], Step [130/395], Loss: 0.8171, Accuracy: 75.43%\n",
      "Epoch [2/3], Step [140/395], Loss: 0.8734, Accuracy: 75.13%\n",
      "Epoch [2/3], Step [150/395], Loss: 1.8777, Accuracy: 74.79%\n",
      "Epoch [2/3], Step [160/395], Loss: 1.5057, Accuracy: 74.73%\n",
      "Epoch [2/3], Step [170/395], Loss: 0.9345, Accuracy: 74.78%\n",
      "Epoch [2/3], Step [180/395], Loss: 1.6460, Accuracy: 74.83%\n",
      "Epoch [2/3], Step [190/395], Loss: 1.3412, Accuracy: 74.93%\n",
      "Epoch [2/3], Step [200/395], Loss: 1.4313, Accuracy: 75.00%\n",
      "Epoch [2/3], Step [210/395], Loss: 1.0540, Accuracy: 74.97%\n",
      "Epoch [2/3], Step [220/395], Loss: 0.5233, Accuracy: 75.17%\n",
      "Epoch [2/3], Step [230/395], Loss: 2.0262, Accuracy: 75.35%\n",
      "Epoch [2/3], Step [240/395], Loss: 1.5732, Accuracy: 75.18%\n",
      "Epoch [2/3], Step [250/395], Loss: 1.2964, Accuracy: 75.22%\n",
      "Epoch [2/3], Step [260/395], Loss: 1.0437, Accuracy: 75.34%\n",
      "Epoch [2/3], Step [270/395], Loss: 1.8973, Accuracy: 75.14%\n",
      "Epoch [2/3], Step [280/395], Loss: 1.8189, Accuracy: 75.09%\n",
      "Epoch [2/3], Step [290/395], Loss: 1.6466, Accuracy: 74.94%\n",
      "Epoch [2/3], Step [300/395], Loss: 1.7762, Accuracy: 74.83%\n",
      "Epoch [2/3], Step [310/395], Loss: 1.0290, Accuracy: 74.90%\n",
      "Epoch [2/3], Step [320/395], Loss: 1.2533, Accuracy: 74.94%\n",
      "Epoch [2/3], Step [330/395], Loss: 2.4031, Accuracy: 74.98%\n",
      "Epoch [2/3], Step [340/395], Loss: 1.6807, Accuracy: 74.87%\n",
      "Epoch [2/3], Step [350/395], Loss: 1.2839, Accuracy: 74.95%\n",
      "Epoch [2/3], Step [360/395], Loss: 0.8767, Accuracy: 74.98%\n",
      "Epoch [2/3], Step [370/395], Loss: 0.4547, Accuracy: 75.08%\n",
      "Epoch [2/3], Step [380/395], Loss: 0.4947, Accuracy: 75.16%\n",
      "Epoch [2/3], Step [390/395], Loss: 1.3864, Accuracy: 75.11%\n",
      "Epoch [3/3], Step [10/395], Loss: 1.6059, Accuracy: 75.62%\n",
      "Epoch [3/3], Step [20/395], Loss: 2.1270, Accuracy: 75.62%\n",
      "Epoch [3/3], Step [30/395], Loss: 0.9543, Accuracy: 76.67%\n",
      "Epoch [3/3], Step [40/395], Loss: 0.9183, Accuracy: 77.97%\n",
      "Epoch [3/3], Step [50/395], Loss: 0.7700, Accuracy: 77.75%\n",
      "Epoch [3/3], Step [60/395], Loss: 0.9265, Accuracy: 77.29%\n",
      "Epoch [3/3], Step [70/395], Loss: 1.8197, Accuracy: 77.14%\n",
      "Epoch [3/3], Step [80/395], Loss: 1.3135, Accuracy: 77.03%\n",
      "Epoch [3/3], Step [90/395], Loss: 1.1076, Accuracy: 76.94%\n",
      "Epoch [3/3], Step [100/395], Loss: 1.1061, Accuracy: 76.62%\n",
      "Epoch [3/3], Step [110/395], Loss: 1.0829, Accuracy: 76.76%\n",
      "Epoch [3/3], Step [120/395], Loss: 1.1707, Accuracy: 76.98%\n",
      "Epoch [3/3], Step [130/395], Loss: 1.1766, Accuracy: 76.63%\n",
      "Epoch [3/3], Step [140/395], Loss: 1.4692, Accuracy: 76.34%\n",
      "Epoch [3/3], Step [150/395], Loss: 1.1314, Accuracy: 76.46%\n",
      "Epoch [3/3], Step [160/395], Loss: 1.7151, Accuracy: 76.56%\n",
      "Epoch [3/3], Step [170/395], Loss: 1.4644, Accuracy: 76.58%\n",
      "Epoch [3/3], Step [180/395], Loss: 1.5624, Accuracy: 76.08%\n",
      "Epoch [3/3], Step [190/395], Loss: 1.9078, Accuracy: 76.02%\n",
      "Epoch [3/3], Step [200/395], Loss: 0.9977, Accuracy: 76.03%\n",
      "Epoch [3/3], Step [210/395], Loss: 1.6475, Accuracy: 75.86%\n",
      "Epoch [3/3], Step [220/395], Loss: 1.4470, Accuracy: 75.68%\n",
      "Epoch [3/3], Step [230/395], Loss: 0.9767, Accuracy: 75.33%\n",
      "Epoch [3/3], Step [240/395], Loss: 1.2042, Accuracy: 75.44%\n",
      "Epoch [3/3], Step [250/395], Loss: 0.9449, Accuracy: 75.75%\n",
      "Epoch [3/3], Step [260/395], Loss: 2.3048, Accuracy: 75.84%\n",
      "Epoch [3/3], Step [270/395], Loss: 2.9505, Accuracy: 75.62%\n",
      "Epoch [3/3], Step [280/395], Loss: 1.3123, Accuracy: 75.38%\n",
      "Epoch [3/3], Step [290/395], Loss: 1.4221, Accuracy: 75.26%\n",
      "Epoch [3/3], Step [300/395], Loss: 0.4333, Accuracy: 75.42%\n",
      "Epoch [3/3], Step [310/395], Loss: 1.4322, Accuracy: 75.48%\n",
      "Epoch [3/3], Step [320/395], Loss: 1.1703, Accuracy: 75.31%\n",
      "Epoch [3/3], Step [330/395], Loss: 1.0358, Accuracy: 75.15%\n",
      "Epoch [3/3], Step [340/395], Loss: 1.3708, Accuracy: 75.15%\n",
      "Epoch [3/3], Step [350/395], Loss: 1.2141, Accuracy: 75.11%\n",
      "Epoch [3/3], Step [360/395], Loss: 1.7606, Accuracy: 75.19%\n",
      "Epoch [3/3], Step [370/395], Loss: 1.8509, Accuracy: 74.97%\n",
      "Epoch [3/3], Step [380/395], Loss: 1.3009, Accuracy: 75.08%\n",
      "Epoch [3/3], Step [390/395], Loss: 1.1504, Accuracy: 75.22%\n",
      "Test Accuracy: 58.17%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from transformers import SwinForImageClassification\n",
    "\n",
    "# Define hyperparameters\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 3  # Reduced to 3 epochs\n",
    "NUM_CLASSES = 101  # UCF101 dataset classes\n",
    "TEST_SPLIT = 0.2  # Proportion of data used for testing\n",
    "\n",
    "# Define data transformations for pre-processing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Load datasets from separate train and test folders\n",
    "train_dataset = datasets.ImageFolder(root=\"/Users/rukmini/Documents/Project/newdata/reduced_train\", transform=transform)\n",
    "test_dataset = datasets.ImageFolder(root=\"/Users/rukmini/Documents/Project/newdata/reduced_test\", transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "# Load Swin Transformer model\n",
    "model = SwinForImageClassification.from_pretrained(\n",
    "    \"microsoft/swin-tiny-patch4-window7-224\",\n",
    "    num_labels=NUM_CLASSES,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Training loop\n",
    "def train_model():\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images).logits\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Statistics\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{EPOCHS}], Loss: {running_loss / len(train_loader):.4f}, \"\n",
    "              f\"Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "# Testing loop\n",
    "def test_model():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images).logits\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "# Real-time updates during training\n",
    "def real_time_updates():\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images).logits\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Statistics\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "            # Update every batch\n",
    "            if (i + 1) % 10 == 0:  # Update every 10 batches\n",
    "                print(f\"Epoch [{epoch + 1}/{EPOCHS}], Step [{i + 1}/{len(train_loader)}], \"\n",
    "                      f\"Loss: {loss.item():.4f}, Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "# Train and test the model\n",
    "if __name__ == \"__main__\":\n",
    "    real_time_updates()\n",
    "    test_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375e7052",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f77bb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2715c2e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SwinForImageClassification were not initialized from the model checkpoint at microsoft/swin-tiny-patch4-window7-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([101, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([101]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from transformers import SwinForImageClassification\n",
    "\n",
    "\n",
    "\n",
    "# Define hyperparameters\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 20\n",
    "NUM_CLASSES = 101  # UCF101 dataset classes\n",
    "TEST_SPLIT = 0.2  # Proportion of  data used for testing\n",
    "\n",
    "\n",
    "\n",
    "# Define data transformations for pre-processing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# Load datasets from separate train and test folders\n",
    "train_dataset = datasets.ImageFolder(root=\"/Users/rukmini/Documents/Project/newdata/reduced_train\", transform=transform)\n",
    "test_dataset = datasets.ImageFolder(root=\"/Users/rukmini/Documents/Project/newdata/reduced_test\", transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load Swin Transformer model\n",
    "model = SwinForImageClassification.from_pretrained(\n",
    "    \"microsoft/swin-tiny-patch4-window7-224\",\n",
    "    num_labels=NUM_CLASSES,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "\n",
    "\n",
    "# Training loop\n",
    "def train_model():\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images).logits\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Statistics\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{EPOCHS}], Loss: {running_loss / len(train_loader):.4f}, \"\n",
    "              f\"Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Testing loop\n",
    "def test_model():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images).logits\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "\n",
    "\n",
    "# Real-time updates during training\n",
    "def real_time_updates():\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images).logits\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Statistics\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "            # Update every batch\n",
    "            if (i + 1) % 10 == 0:  # Update every 10 batches\n",
    "                print(f\"Epoch [{epoch + 1}/{EPOCHS}], Step [{i + 1}/{len(train_loader)}], \"\n",
    "                      f\"Loss: {loss.item():.4f}, Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "\n",
    "\n",
    "# Train and test the model\n",
    "if __name__ == \"__main__\":\n",
    "    real_time_updates()\n",
    "    test_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a430512a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
